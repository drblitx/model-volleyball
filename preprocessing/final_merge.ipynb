{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "875c62a6",
   "metadata": {},
   "source": [
    "### Stats and Schedule Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "548180fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module imports\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c219af8e",
   "metadata": {},
   "source": [
    "#### Pre-Cleaning Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75aa1046",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEDULE_PATH = \"../data/cleaned_master_schedule.csv\"\n",
    "STATS_PATH = \"../data/cleaned/all_stats_merged.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfb2d7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    print(\"Loading...\")\n",
    "    schedule_df = pd.read_csv(SCHEDULE_PATH)\n",
    "    stats_df = pd.read_csv(STATS_PATH)\n",
    "    print(f\"Loaded {len(schedule_df)} rows from schedule\")\n",
    "    print(f\"Loaded {len(stats_df)} rows from stats\")\n",
    "    print()\n",
    "    return schedule_df, stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d34be8",
   "metadata": {},
   "source": [
    "#### Merge Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66987330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_match_keys(schedule_df, stats_df):\n",
    "    schedule_keys = set(schedule_df[\"match_key\"])\n",
    "    stats_keys = set(stats_df[\"match_key\"])\n",
    "\n",
    "    only_in_schedule = sorted(schedule_keys - stats_keys)\n",
    "    only_in_stats = sorted(stats_keys - schedule_keys)\n",
    "\n",
    "    if not only_in_schedule and not only_in_stats:\n",
    "        print(\"‚úÖ match_keys ALIGNED between schedule and stats.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è match_keys MISMATCH!\")\n",
    "\n",
    "        if only_in_schedule:\n",
    "            print(f\"  üü° {len(only_in_schedule)} match_keys only in schedule:\")\n",
    "            for key in only_in_schedule:\n",
    "                print(f\" - {key}\")\n",
    "\n",
    "        if only_in_stats:\n",
    "            print(f\"  üî¥ {len(only_in_stats)} match_keys only in stats:\")\n",
    "            for key in only_in_stats:\n",
    "                print(f\" - {key}\")\n",
    "\n",
    "    return not only_in_schedule and not only_in_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1e71179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_stat_integrity(stats_df, cleaned_dir=\"../data/cleaned\"):\n",
    "    print(\"\\nüîç Checking stat integrity...\")\n",
    "\n",
    "    seasons = {\n",
    "        \"FR\": \"freshman_stats_merged.csv\",\n",
    "        \"SO\": \"sophomore_stats_merged.csv\",\n",
    "        \"SR\": \"senior_stats_merged.csv\"\n",
    "    }\n",
    "\n",
    "    all_good = True\n",
    "\n",
    "    stats_df = stats_df[stats_df[\"result\"].notna()]\n",
    "\n",
    "    for season_code, filename in seasons.items():\n",
    "        file_path = os.path.join(cleaned_dir, filename)\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"‚ùå season stats file missing: {filename}\")\n",
    "            all_good = False\n",
    "            continue\n",
    "\n",
    "        season_df = pd.read_csv(file_path)\n",
    "        merged_df = stats_df[stats_df[\"season\"] == season_code]\n",
    "\n",
    "        season_df = season_df.set_index(\"match_key\")\n",
    "        merged_df = merged_df.set_index(\"match_key\")\n",
    "\n",
    "        meta_cols = [\"date\", \"result\", \"opponent\", \"season\", \"opponent_slug\", \"sets_played\", \"match_no\"]\n",
    "        season_df = season_df.drop(columns=[c for c in meta_cols if c in season_df.columns], errors=\"ignore\")\n",
    "        merged_df = merged_df.drop(columns=[c for c in meta_cols if c in merged_df.columns], errors=\"ignore\")\n",
    "\n",
    "        season_df.index = season_df.index.astype(str)\n",
    "        merged_df.index = merged_df.index.astype(str)\n",
    "        season_df.columns = season_df.columns.astype(str)\n",
    "        merged_df.columns = merged_df.columns.astype(str)\n",
    "\n",
    "        if season_df.index.duplicated().any():\n",
    "            dupes = season_df.index[season_df.index.duplicated()].unique()\n",
    "            print(f\"‚ùå match_keys in season_df for {season_code}: {list(dupes)} has duplicates\")\n",
    "            all_good = False\n",
    "            continue\n",
    "\n",
    "        if merged_df.index.duplicated().any():\n",
    "            dupes = merged_df.index[merged_df.index.duplicated()].unique()\n",
    "            print(f\"‚ùå match_keys in merged_df for {season_code}: {list(dupes)} has duplicates\")\n",
    "            all_good = False\n",
    "            continue\n",
    "\n",
    "        common_cols = sorted(season_df.columns.intersection(merged_df.columns))\n",
    "        common_index = sorted(season_df.index.intersection(merged_df.index))\n",
    "\n",
    "        season_df = season_df.reindex(index=common_index, columns=common_cols)\n",
    "        merged_df = merged_df.reindex(index=common_index, columns=common_cols)\n",
    "\n",
    "        for col in common_cols:\n",
    "            try:\n",
    "                merged_df[col] = merged_df[col].astype(season_df[col].dtype)\n",
    "            except Exception:\n",
    "                merged_df[col] = merged_df[col].astype(float)\n",
    "\n",
    "        try:\n",
    "            season_df = season_df[common_cols].sort_index()\n",
    "            merged_df = merged_df[common_cols].sort_index()\n",
    "\n",
    "            print(f\"[{season_code}] comparing {season_df.shape} vs {merged_df.shape}\")\n",
    "\n",
    "            diffs = season_df.compare(merged_df, align_axis=0)\n",
    "            if not diffs.empty:\n",
    "                print(f\"‚ö†Ô∏è mismatch found in {season_code} stats! Showing up to 10 differences:\")\n",
    "                print(diffs.head(10))\n",
    "\n",
    "                sample_key = diffs.index.get_level_values(0)[0]\n",
    "                print(\"üß™ match_key:\", sample_key)\n",
    "                print(\"Season file:\\n\", season_df.loc[sample_key])\n",
    "                print(\"All stats merged:\\n\", merged_df.loc[sample_key])\n",
    "\n",
    "                all_good = False\n",
    "            else:\n",
    "                print(f\"‚úÖ stats for {season_code} match exactly.\")\n",
    "        except ValueError as ve:\n",
    "            print(f\"‚ùå comparision error in {season_code}: {ve}\")\n",
    "            print(f\"  üü° in {filename} only: {sorted(set(season_df.columns) - set(merged_df.columns))}\")\n",
    "            print(f\"  üî¥ in all_stats_merged only: {sorted(set(merged_df.columns) - set(season_df.columns))}\")\n",
    "            all_good = False\n",
    "\n",
    "    return all_good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4df91e",
   "metadata": {},
   "source": [
    "#### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ccfbf04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading...\n",
      "Loaded 157 rows from schedule\n",
      "Loaded 157 rows from stats\n",
      "\n",
      "‚úÖ match_keys ALIGNED between schedule and stats.\n",
      "\n",
      "üîç Checking stat integrity...\n",
      "[FR] comparing (36, 0) vs (36, 0)\n",
      "‚úÖ stats for FR match exactly.\n",
      "[SO] comparing (36, 0) vs (36, 0)\n",
      "‚úÖ stats for SO match exactly.\n",
      "[SR] comparing (46, 0) vs (46, 0)\n",
      "‚úÖ stats for SR match exactly.\n",
      "\n",
      "üéØ checks passed. proceeding to final merge...\n",
      "\n",
      "merging schedule and stats...\n",
      "‚úÖ merged dataset: 157 matches, 71 columns\n",
      "üì¶ saved: ../data/full_merged_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "schedule_df, stats_df = load_data()\n",
    "\n",
    "if not compare_match_keys(schedule_df, stats_df):\n",
    "    print(\"‚ùå match_key mismatch. fix before continuing.\")\n",
    "    exit(1)\n",
    "\n",
    "if not validate_stat_integrity(stats_df):\n",
    "    print(\"‚ùå stat mismatches found. resolve before proceeding.\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"\\nüéØ checks passed. proceeding to final merge...\")\n",
    "\n",
    "print(\"\\nmerging schedule and stats...\")\n",
    "\n",
    "# drop duplicate stat columns that exist in schedule\n",
    "merged_df = schedule_df.merge(stats_df, on=\"match_key\", how=\"left\")\n",
    "\n",
    "redundant_cols = [\"date_y\", \"opponent_y\", \"season_y\", \"result_y\", \"sets_played_y\"]\n",
    "merged_df = merged_df.drop(columns=[col for col in redundant_cols if col in merged_df.columns], errors=\"ignore\")\n",
    "\n",
    "# rename cleanly if needed\n",
    "merged_df = merged_df.rename(columns={\n",
    "    \"date_x\": \"date\",\n",
    "    \"opponent_x\": \"opponent\",\n",
    "    \"season_x\": \"season\",\n",
    "    \"result_x\": \"result\",\n",
    "    \"sets_played_x\": \"sets_played\"\n",
    "})\n",
    "\n",
    "if \"career_match_index\" in merged_df.columns:\n",
    "    merged_df[\"career_match_index\"] = merged_df[\"career_match_index\"].astype(\"Int64\")\n",
    "\n",
    "# reorder columns for readability\n",
    "core_order = [\n",
    "    \"match_key\", \"career_match_index\", \"career_stage\", \"season\", \"season_match_number\",\n",
    "    \"date\", \"day_of_week\", \"week_of_season\", \"days_since_last_match\", \"match_no\", \"total_matches_that_day\", \n",
    "    \"total_sets_that_day\", \"multi_game_day\", \"first_match_of_day\", \"last_match_of_day\", \"same_day_opponent_num\",\n",
    "    \"opponent\", \"opponent_slug\", \"season_opponent_num\", \"is_repeat_opponent\", \"deaf_school\", \"match_type\", \"event_name\", \n",
    "    \"milestone_flag\", \"result\", \"set_scores\", \"set_result\", \"set_count\", \"set_diff\", \"was_set_swept\", \"swept_opponent\",\n",
    "     \"win_streak\", \"loss_streak\", \"comeback_win\", \"total_points_for\", \"total_points_against\", \"margin_pct\", \n",
    "    \"high_margin_win\", \"low_margin_loss\", \"location\", \"did_play\",\n",
    "]\n",
    "\n",
    "stat_cols = [\n",
    "    \"sets_played\", \"kills\", \"kills_per_set\", \"kill_pct\",\n",
    "    \"kill_attempts\", \"kill_errors\", \"hit_pct\",\n",
    "    \"assists\", \"assists_per_set\", \"ball_handling_attempts\", \"ball_handling_errors\",\n",
    "    \"solo_blocks\", \"assisted_blocks\", \"total_blocks\", \"blocks_per_set\", \"block_errors\",\n",
    "    \"digs\", \"dig_errors\", \"digs_per_set\",\n",
    "    \"receiving\", \"receiving_errors\", \"receiving_per_set\",\n",
    "    \"aces\", \"aces_per_set\", \"ace_pct\", \n",
    "    \"serve_attempts\", \"serve_errors\", \"serve_pct\", \"points\"\n",
    "]\n",
    "\n",
    "remaining = [col for col in merged_df.columns if col not in core_order + stat_cols + [\"maxpreps\"]]\n",
    "\n",
    "final_order = core_order + stat_cols + remaining + [\"maxpreps\"]\n",
    "\n",
    "merged_df = merged_df[[col for col in final_order if col in merged_df.columns]]\n",
    "\n",
    "print(f\"‚úÖ merged dataset: {len(merged_df)} matches, {merged_df.shape[1]} columns\")\n",
    "\n",
    "merged_path = \"../data/full_merged_dataset.csv\"\n",
    "merged_df.to_csv(merged_path, index=False)\n",
    "print(f\"üì¶ saved: {merged_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "school",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
